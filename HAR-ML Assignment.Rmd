---
title: "Human Activity Recognition Machine Learning Assignment"
author: "Geoff Skellams"
date: "5 December 2017"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align="center", fig.height = 3.5)

# import the libraries we'll need later on
library(caret)
```

## Synopsis
The increased adoption of wearable computers and fitness trackers has led to a boon for researchers looking into human movement. This paper explores data captured from a set of weightlifters, who wore fitness trackers and had their movements tracked while performing a range of weightlifting exercises. Each activity in the training set was categorised into one of five separate classses, indicating whether the activity was performed correctly or not. By using a range of machine learning techniques, we explore whether the computer can determine which class each exercise belongs in, from the data alone. We then go on to predict the classes of a much smaller test dataset.

## Data Exploration

Before the data can be analysed, it must first be loaded into memory. If the file do not currently exist in the working directory, they will be automatically downloaded.

Initial exploratory data analysis showed that many of the columns were filled with either blanks, NA values or divide by zero errors. In order to simplify the data cleansing process, all of these values were converted to NA during the file read operation.

```{r dataLoad}
# download the training and testing files, if required.
if (!file.exists("pml-training.csv"))
{
    url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(url, dest = "./pml-training.csv", mode = "w") 
    
}

if (!file.exists("pml-testing.csv"))
{
    url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(url, dest = "./pml-testing.csv", mode = "w") 
    
}

# read the training data into a dataframe. Set as NA anything marked as blank, "NA", or a division by zero error
fullTrainingSet <- read.csv("pml-training.csv", na.strings = c("", "NA","#DIV/0!"))
fullTestingSet <- read.csv("pml-testing.csv", na.strings = c("", "NA", "#DIV/0!")) 

dim(fullTrainingSet)
dim(fullTestingSet)
```

## Data Analysis

### Utility Functions
To simplify the code structure, much of the data cleansing code was written as two separate functions. 

The first function, **rejectOnNAPercentage()** takes in a single column from a data set and calculates the percentage of values in that column that are NA. If that value is greater than or equal to a threshold percentage passed into the function, it will return TRUE, indicating that the column should be rejected

```{r rejectOnNAPercentage}
#---------------------------------------------------------------------------------------------
# Function Name:    rejectOnNAPercentage()
# Function Summary: Returns true if the percentage of values in a column which are NA is above
#                   the threshold value
#---------------------------------------------------------------------------------------------
rejectOnNAPercentage <- function(dataColumn, thresholdVal)
{
    naPercentage <- sum(is.na(dataColumn))/length(dataColumn)
    retValue <- FALSE
    if (naPercentage >= thresholdVal)
    {
        retValue <- TRUE
    }
    retValue
}
```

The other function, **getColsToRemove()**, dramatically reduces the number of variables in the dataset to make the machine learning process easier. Three criteria were used to remove columns from the dataset:

 1. The first seven columns in each row form a unique identifier for that row, detailing who the participant was, and timestamps for when the exercise was performed. These values are irrelevant for the machine learning process, so they will all be removed.
 2. As previously mentioned, there are a large number of columns which are mostly comprised of NA values. Any column which has more than 50% NA values will be removed from the dataset; and
 3. Some columns have next to no variance in its values. These columns provide no real benefit to the machine learning process, so these columns will also be removed.

```{r getColsToRemove}
#---------------------------------------------------------------------------------------------
# Function Name:    getColsToRemove()
# Function Summary: This function will returning a list of column names to be
#                   removed from a dataset in order to make it useful for the machine learning
#                   algorithm.
#---------------------------------------------------------------------------------------------
getColsToRemove <- function(dataset)
{
    # drop the first seven columns because they only contain information about the subject and 
    # timestamps, which are not relevant to the learning process
    colsToRemove <- colnames(dataset)[1:7]
    
    # remove any column where the NA percentage is more than 50%
    highNACols <- sapply(dataset, rejectOnNAPercentage, thresholdVal = 0.5)
    highNAColNames <- colnames(dataset)[highNACols]
    colsToRemove <- append(colsToRemove, highNAColNames)
    
    # remove any column with low variance
    lowVar <- nearZeroVar(dataset, saveMetrics = TRUE)
    lowVarNames <- colnames(dataset)[lowVar$nzv]
    colsToRemove <- append(colsToRemove, lowVarNames)
    
    # return the unique list of column names
    unique(colsToRemove)
}
```

### Data Cleansing and Preparation

Before we can train the machine learning algorithms on the data, we need to clean the full training and testing datasets, removing the unnessary columns. The same set of columns are removed from both the training and test datasets, to ensure that the prediction on the test set will run smoothly. As can be seen, this reduces the number of columns from 160 to only 53.

```{r dataCleansing}
set.seed(5122017)

# get the list of columns to remove
colNamesToRemove <- getColsToRemove(fullTrainingSet)

# work out which columns to leave in
colsToRemove <- colnames(fullTrainingSet) %in% colNamesToRemove

# remove those columns from the training and test sets
cleanTrainingSet <- fullTrainingSet[, !colsToRemove]
cleanTestSet <- fullTestingSet[, !colsToRemove]

dim(cleanTrainingSet)
dim(cleanTestSet)

```

### Data Partitioning

One final step needs to be performed before training the algorithms: partitioning the full training dataset into a training set and a test set, which can be used for cross-validation and error rate measuring. The training dataset will be 80% of the full training set, while the test set will be the remaining 20%

```{r dataPartitioning}
# partition the training data into a training (80%) and test (20%) sets
inTrain <- createDataPartition(y = cleanTrainingSet$classe, p = 0.8, list = FALSE)
trainingData <- cleanTrainingSet[inTrain,]
testingData <- cleanTrainingSet[-inTrain,]

dim(trainingData)
dim(testingData)
```

## Machine Learning

Three different machine learning algorithms were trained with the training dataset, and then validated with the testing set. These algorithms are:

 1. Decision Tree
 2. Boosting
 3. Random Forest

In each case, the code performs the training with the training set, using the **Classe** variable as the outcome, and the other variables as the predictor. The code then performs a prediction using the testing set. It will then show the confusion matrix for the predicted data, while shows how accurate the algorithm was at predicting the class of each observation, compared with the actual recorded class.

### Decision Tree

```{r decisionTree}
decTreeModel <- train(classe ~ ., data = trainingData, method = "rpart")
decTreePred <- predict(decTreeModel, newdata = testingData)
confusionMatrix(decTreePred, testingData$classe)
```

The Decision Tree algorithm was not particularly effective at predicting the correct outcome after training. As can be seen from the confusion matrix, the prediction accuracy was only about 50%. 

The Kappa value is particularly low at 0.28, indicating that the algorithm is only approximately 28% better than random chance at correctly categorising each observation.

### Boosting
```{r boosting}
boostModel <- train(classe ~ ., data = trainingData, method = "gbm", verbose = FALSE)
boostPred <- predict(boostModel, newdata = testingData)
confusionMatrix(boostPred, testingData$classe)

```

The Boosting algorithm was *far* more effective than the decision tree in correctly predicting the class variable from the data, getting the correct answer in 96% of cases.

Boosting's Kappa value was 95.3%, which is very close to the model's accuracy, indicating that it will successfully predict the outcome in the vast majority of cases with a high degree of confidence.

### Random Forest

```{r randomForest}
rdmForestModel <- train(classe ~ ., data = trainingData, method = "rf")
rdmForestPred <- predict(rdmForestModel, newdata = testingData)
confusionMatrix(rdmForestPred, testingData$classe)
```

The Random Forest algorithm was the most successful of the three algorithms used, correctly predicting the outcome in 99.4% of cases.

Random Forest's Kappa value was 99.2%, indicating that it too is very successful when compared with random chance prediction.

## Final Prediction

Because the Random Forest algorithm had the highest accuracy rating of the three machine learning algorithms tested, we used it first to perform the prediction on the real testing dataset:

```{r cleanRdmForestPred}
cleanRdmForestPred <- predict(rdmForestModel, newdata = cleanTestSet)
cleanRdmForestPred
```

As a secondary form of validation, we also predicted the test set's classes using the Boosting model:

```{r cleanBoostPred}
cleanBoostPred <- predict(boostModel, newdata = cleanTestSet)
cleanBoostPred
```

Both algorithms predicted the test data in exactly the same way, so we can be reasonably confident that the predictions are correct.

## Reference

For more information about the training and testing dataset, please see:

> Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. **Qualitative Activity Recognition of Weight Lifting
> Exercises.** Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart,
> Germany: ACM SIGCHI, 2013.
